{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"xlnet_davidson.ipynb","provenance":[{"file_id":"12LQt7sUJfeK3PfzaGo1q6eg8obpwfxCd","timestamp":1654035473389},{"file_id":"1YWS5XdP18yaAIiz_X3DBmTRSO6aaB8hY","timestamp":1654033305696},{"file_id":"10f0VcONTOGZ9sHIW4JQZzFEWsD44k4xQ","timestamp":1654000136899}],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyOsjsgKLddy7fB9ZcAQ45hb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fa-AmHtICVOb","executionInfo":{"status":"ok","timestamp":1654035659752,"user_tz":-180,"elapsed":6030,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"75694d32-8550-4cca-b775-22aa416612f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from keras.models import load_model\n","from keras.metrics import Precision, Recall\n","from keras.callbacks import CSVLogger, EarlyStopping\n","from keras.utils import np_utils\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","\n","!pip install transformers\n","!pip install sentencepiece\n","from transformers import XLNetTokenizer, TFXLNetModel, PreTrainedTokenizerBase\n"]},{"cell_type":"code","source":["import nltk\n","import matplotlib.pyplot as plt\n","\n","from re import sub\n","from pandas import DataFrame\n","from tabulate import tabulate\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from typing import Union, List, Tuple\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from pandas.io.parsers import TextFileReader\n","from collections import Counter\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","__STOPWORDS = stopwords.words(\"english\")\n","STEMMER = PorterStemmer()\n","LEMMATIZER = WordNetLemmatizer()\n","\n","\n","def process_data(text: str, do_stemming: bool = False, do_lemmas: bool = False, do_lowercase: bool = False) -> str:\n","    \"\"\"\n","    @param text: The text to process. It will remove the money amounts, retweets, links,\n","                 hashtags, punctuation and it will lowercase all the words\n","    @param do_stemming: Steam words to have less in vocabulary if set to true\n","    @param do_lowercase: Lowercase input text if set to true\n","    @param do_lemmas: Lemmatize word to be at a dictionary representation if true\n","    @return: The new processed text as a list of words\n","    \"\"\"\n","    text = sub(r\" +\", \" \", text)\n","    text = sub(r\"\\S@\\S\\s?\", \"\", text)\n","    text = sub(r\"[0-9]+(?:.[0-9]+){3}\", \"\", text)\n","    text = sub(r\"\\$\\w*\", \"\", text)\n","    text = sub(r\"(RT)+\", \"\", text)\n","    text = sub(r\"(lt)+\", \"\", text)\n","    text = sub(r\"(gt)+\", \"\", text)\n","    text = sub(r\"@[a-zA-Z0-9\\_]+\", \"\", text)\n","    text = sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\", \"\", text)\n","    text = sub(r\"#\", \"\", text)\n","    text = sub(r\"\\d+\", \"\", text)\n","    text = sub(\"[^A-Za-z0-9]+\", \" \", text)  # also removes special characters since they are not alphanumeric\n","    if do_lowercase:\n","        text = text.lower()\n","\n","    # tokenized_data = __TOKENIZER.tokenize(text)\n","    tokenized_data = word_tokenize(text)\n","    processed_data = []\n","\n","    for word in tokenized_data:\n","        if word not in __STOPWORDS and word not in punctuation:\n","            if do_stemming and not do_lemmas:\n","                word = STEMMER.stem(word)\n","            elif do_lemmas and not do_stemming:\n","                word = LEMMATIZER.lemmatize(word)\n","            elif do_lemmas and do_stemming:\n","                raise ValueError(\n","                    f\"Can't do both lemmatizing and stemming. Values for do_lemmas={do_lemmas} \"\n","                    f\"and do_stemming={do_stemming} cannot be true for both.\"\n","                )\n","            processed_data.append(word)\n","\n","    return \" \".join(processed_data)\n","\n","\n","def plot_train_data(csv_data: Union[TextFileReader, DataFrame], train_metric: str, validation_metric: str) -> None:\n","    plt.figure()\n","    plt.plot(csv_data[train_metric], color=\"blueviolet\")\n","    plt.plot(csv_data[validation_metric], color=\"green\")\n","    plt.title(f\"{train_metric.capitalize()} over epochs\")\n","    plt.legend([\"train\", \"validation\"])\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(train_metric)\n","    plt.grid(visible=True)\n","\n","\n","def display_train_report_and_f1_score(csv_data: Union[TextFileReader, DataFrame]) -> None:\n","    headers = [\"epoch\", \"accuracy\", \"loss\", \"precision\", \"recall\"]\n","    train_rep = csv_data[csv_data[\"epoch\"] == len(csv_data) - 1][headers]\n","    precision = train_rep[\"precision\"].iloc[0]\n","    recall = train_rep[\"recall\"].iloc[0]\n","    f1_score = 2 * precision * recall / (precision + recall)\n","    train_rep[\"f1_score\"] = f1_score\n","    rep_data = train_rep.values.tolist()\n","    print('\\033[92m')\n","    print(\"╒═════════════════╕\")\n","    print(\"│ Training Report │\")\n","    print(tabulate(rep_data, headers=[header.capitalize() for header in headers] + [\"F1 Score\"], tablefmt=\"fancy_grid\"))\n","\n","\n","def display_readable_time(start_time: float, end_time: float) -> None:\n","    minutes = (end_time - start_time) / 60\n","    ss = (end_time - start_time) % 60\n","    hh = minutes / 60\n","    mm = minutes % 60\n","    print('\\033[94m')\n","    print(f\"Training time: %02d:%02d:%02d | {round(end_time - start_time, 2)} seconds\" % (hh, mm, ss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9H6BKMiY9FE","executionInfo":{"status":"ok","timestamp":1654035660133,"user_tz":-180,"elapsed":386,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"034f13e6-3c6c-4500-82ce-3f921d1a7c4b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4y4JAg_ZR8d","executionInfo":{"status":"ok","timestamp":1654035680943,"user_tz":-180,"elapsed":20812,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"58fd419f-dee9-42dd-cdd0-f47208a12c8f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!mkdir train_local\n","# !wget /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","!unzip /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip -d /content/train_local"],"metadata":{"id":"WJumPHUiZWr8","executionInfo":{"status":"ok","timestamp":1654035680944,"user_tz":-180,"elapsed":34,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc59aa27-fc5e-4c10-b778-c22f6168014c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically Generated Hate Dataset - annotation guidelines.pdf  \n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically_Generated_Hate_Dataset_v0.2.2.csv  \n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically_Generated_Hate_Dataset_v0.2.3.csv  \n","  inflating: /content/train_local/dynamically-hate-vidgen/README.md  \n","  inflating: /content/train_local/hateful-wassem/README.md  \n","  inflating: /content/train_local/hateful-wassem/wassem_hovy_naacl.tsv  \n","   creating: /content/train_local/hatespeech-davidson/data/\n","  inflating: /content/train_local/hatespeech-davidson/data/labeled_data.csv  \n","  inflating: /content/train_local/hatespeech-davidson/data/labeled_data.p  \n","  inflating: /content/train_local/hatespeech-davidson/data/readme.md  \n","  inflating: /content/train_local/hatespeech-davidson/LICENSE  \n","  inflating: /content/train_local/hatespeech-davidson/README.md  \n","  inflating: /content/train_local/large-founta/dev.tsv  \n","  inflating: /content/train_local/large-founta/README.md  \n","  inflating: /content/train_local/large-founta/test.tsv  \n","  inflating: /content/train_local/large-founta/train.tsv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_test_data_v0.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_test_data_v1.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_train_data_v0.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_train_data_v1.csv  \n","  inflating: /content/train_local/olid-zampieri/olid-annotation.txt  \n","  inflating: /content/train_local/olid-zampieri/olid-training-v1.0.tsv  \n","  inflating: /content/train_local/olid-zampieri/README.txt  \n","  inflating: /content/train_local/olid-zampieri/testset-levela.tsv  \n"]}]},{"cell_type":"code","source":["# TODO: Update these with the google stuff\n","DAVIDSON_DIR = os.path.join(\"/content/train_local\", \"hatespeech-davidson\")\n","DAVIDSON_DIR = os.path.join(DAVIDSON_DIR, \"data\")\n","DATASET_PATH = os.path.join(DAVIDSON_DIR, \"labeled_data.csv\")\n","DAVIDSON_MODEL_LOGS_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/logs\", \"davidson\")\n","DAVIDSON_MODEL_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/models\", \"davidson\")\n","\n","\n","MODEL_FILE_NAME = \"xlnet_large_founta\"\n","XLNET_TYPE = \"xlnet-large-cased\"  # xlnet-large-cased | xlnet-base-caseded\n","\n","# Clean: 36 | No lowercase: 29 | Lowercase: 28 | Lowercase & Stemming: 28 | Lowercase & Lemmas: 28\n","MAX_PADDING_LENGTH = 28\n","LEARNING_RATE = 2e-5\n","BATCH_SIZE = 32\n","EPOCHS = 10\n"],"metadata":{"id":"GrcgMWcHY9xb","executionInfo":{"status":"ok","timestamp":1654035680945,"user_tz":-180,"elapsed":17,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","def encode_tweet(tweet: str, xlnet_tokenizer: PreTrainedTokenizerBase):\n","    return xlnet_tokenizer.encode_plus(\n","        # tweet,\n","        process_data(tweet, do_stemming=False, do_lemmas=False, do_lowercase=True),\n","        add_special_tokens=True,\n","        max_length=MAX_PADDING_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_attention_mask=True\n","    )\n","\n","\n","def encode_tweets(tweets_text: list, tweets_labels, xlnet_tokenizer: PreTrainedTokenizerBase):\n","    if tweets_labels is not None:\n","        assert len(tweets_text) == len(tweets_labels), f\"Features and labels must have the same lengths. \" \\\n","                                                       f\"Your input ({len(tweets_text)}, {len(tweets_labels)})\"\n","\n","    input_ids = []\n","    token_type_ids = []\n","    attention_masks = []\n","\n","    for tweet in tweets_text:\n","        tweet_for_xlnet = encode_tweet(tweet, xlnet_tokenizer)\n","        input_ids.append(tweet_for_xlnet[\"input_ids\"])\n","        token_type_ids.append(tweet_for_xlnet[\"token_type_ids\"])\n","        attention_masks.append(tweet_for_xlnet[\"attention_mask\"])\n","\n","    if tweets_labels is not None:\n","        assert len(input_ids) == len(token_type_ids) == len(attention_masks) == len(tweets_labels), \\\n","            \"Arrays must have the same length.\"\n","        return np.array(input_ids), np.array(token_type_ids), np.array(attention_masks), np.array(tweets_labels)\n","\n","    return np.array(input_ids), np.array(token_type_ids), np.array(attention_masks)\n","\n","\n","def generate_xlnet_dict(input_ids, token_type_ids, attention_mask) -> dict:\n","    return {\n","        \"input_ids\": input_ids,\n","        \"token_type_ids\": token_type_ids,\n","        \"attention_mask\": attention_mask\n","    }\n","\n","\n","def xlnet_tuning(xlnet_type: str = XLNET_TYPE):\n","    input_ids = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"input_ids\", dtype=\"int32\")\n","    token_type_ids = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"token_type_ids\", dtype=\"int32\")\n","    attention_masks = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"attention_mask\", dtype=\"int32\")\n","\n","    bert_model = TFXLNetModel.from_pretrained(xlnet_type)\n","    encodings = bert_model(input_ids=input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)[0]\n","    last_encoding = tf.squeeze(encodings[:, -1:, :], axis=1)\n","    # last_encoding = tf.keras.layers.Dropout(0.1)(last_encoding)\n","\n","    outputs = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"outputs\")(last_encoding)\n","\n","    temp_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_masks], outputs=[outputs])\n","    temp_model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"accuracy\", Precision(), Recall()]\n","    )\n","\n","    return temp_model"],"metadata":{"id":"lH5r5U4tZtVg","executionInfo":{"status":"ok","timestamp":1654035757256,"user_tz":-180,"elapsed":235,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["tokenizer = XLNetTokenizer.from_pretrained(XLNET_TYPE, do_lower_case=True)\n","\n","df = pd.read_csv(DATASET_PATH, delimiter=\",\")\n","\n","train_texts = df[\"tweet\"].tolist()\n","train_labels = df[\"class\"].tolist()\n","\n","train_labels = np_utils.to_categorical(train_labels)\n","\n","X_train, X_temp, y_train, y_temp = train_test_split(train_texts, train_labels, test_size=0.2)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n","\n","train_ids, train_tokens, train_masks, train_labels = encode_tweets(\n","    tweets_text=X_train,\n","    tweets_labels=y_train,\n","    xlnet_tokenizer=tokenizer\n",")\n","validation_ids, validation_tokens, validation_masks, val_labels = encode_tweets(\n","    tweets_text=X_val,\n","    tweets_labels=y_val,\n","    xlnet_tokenizer=tokenizer\n",")\n","test_ids, test_tokens, test_masks, test_labels = encode_tweets(\n","    tweets_text=X_test,\n","    tweets_labels=y_test,\n","    xlnet_tokenizer=tokenizer\n",")\n","\n","train_data = generate_xlnet_dict(train_ids, train_tokens, train_masks)\n","validation_data = (generate_xlnet_dict(validation_ids, validation_tokens, validation_masks), val_labels)\n","test_data = generate_xlnet_dict(test_ids, test_tokens, test_masks)\n"],"metadata":{"id":"XB_YncKMaZKv","executionInfo":{"status":"ok","timestamp":1654035770782,"user_tz":-180,"elapsed":11864,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model = xlnet_tuning()\n","print(model.summary())\n","early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=2, restore_best_weights=True)\n","csv_logger = CSVLogger(os.path.join(DAVIDSON_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), separator=\",\",\n","                        append=False)\n","start_time = time.time()\n","hist = model.fit(train_data, train_labels, validation_data=validation_data, epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                  callbacks=[csv_logger, early_stop])\n","end_time = time.time()\n","model.save(os.path.join(DAVIDSON_MODEL_PATH, f\"{MODEL_FILE_NAME}.h5\"))\n","display_readable_time(start_time=start_time, end_time=end_time)\n"],"metadata":{"id":"G9_FachvajAk","executionInfo":{"status":"ok","timestamp":1654037743232,"user_tz":-180,"elapsed":1972461,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"35fdafc9-6940-4acd-e0c3-de2ebd5a5683"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at xlnet-large-cased were not used when initializing TFXLNetModel: ['lm_loss']\n","- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-large-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 28)]         0           []                               \n","                                                                                                  \n"," attention_mask (InputLayer)    [(None, 28)]         0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 28)]         0           []                               \n","                                                                                                  \n"," tfxl_net_model_1 (TFXLNetModel  TFXLNetModelOutput(  360268800  ['input_ids[0][0]',              \n"," )                              last_hidden_state=(               'attention_mask[0][0]',         \n","                                None, 28, 1024),                  'token_type_ids[0][0]']         \n","                                 mems=((28, None, 1                                               \n","                                024),                                                             \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024),                                                \n","                                 (28, None, 1024)),                                               \n","                                 hidden_states=None                                               \n","                                , attentions=None)                                                \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 1, 1024)     0           ['tfxl_net_model_1[0][0]']       \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," tf.compat.v1.squeeze_1 (TFOpLa  (None, 1024)        0           ['tf.__operators__.getitem_1[0][0\n"," mbda)                                                           ]']                              \n","                                                                                                  \n"," outputs (Dense)                (None, 3)            3075        ['tf.compat.v1.squeeze_1[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 360,271,875\n","Trainable params: 360,271,875\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/10\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_1/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_1/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","620/620 [==============================] - 310s 453ms/step - loss: 0.8286 - accuracy: 0.7480 - precision_1: 0.7581 - recall_1: 0.7267 - val_loss: 0.4507 - val_accuracy: 0.8632 - val_precision_1: 0.8750 - val_recall_1: 0.8446\n","Epoch 2/10\n","620/620 [==============================] - 274s 441ms/step - loss: 0.4287 - accuracy: 0.8607 - precision_1: 0.8732 - recall_1: 0.8480 - val_loss: 0.3713 - val_accuracy: 0.8935 - val_precision_1: 0.8934 - val_recall_1: 0.8927\n","Epoch 3/10\n","620/620 [==============================] - 274s 441ms/step - loss: 0.3393 - accuracy: 0.8865 - precision_1: 0.8958 - recall_1: 0.8776 - val_loss: 0.3197 - val_accuracy: 0.8923 - val_precision_1: 0.8995 - val_recall_1: 0.8850\n","Epoch 4/10\n","620/620 [==============================] - 273s 440ms/step - loss: 0.3099 - accuracy: 0.8938 - precision_1: 0.9022 - recall_1: 0.8858 - val_loss: 0.3651 - val_accuracy: 0.8805 - val_precision_1: 0.8830 - val_recall_1: 0.8773\n","Epoch 5/10\n","620/620 [==============================] - 273s 441ms/step - loss: 0.2722 - accuracy: 0.9061 - precision_1: 0.9121 - recall_1: 0.9007 - val_loss: 0.2995 - val_accuracy: 0.8991 - val_precision_1: 0.9015 - val_recall_1: 0.8939\n","Epoch 6/10\n","620/620 [==============================] - 273s 440ms/step - loss: 0.2494 - accuracy: 0.9119 - precision_1: 0.9166 - recall_1: 0.9064 - val_loss: 0.3265 - val_accuracy: 0.8931 - val_precision_1: 0.8982 - val_recall_1: 0.8862\n","Epoch 7/10\n","620/620 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9193 - precision_1: 0.9241 - recall_1: 0.9146Restoring model weights from the end of the best epoch: 5.\n","620/620 [==============================] - 273s 441ms/step - loss: 0.2332 - accuracy: 0.9193 - precision_1: 0.9241 - recall_1: 0.9146 - val_loss: 0.3295 - val_accuracy: 0.9052 - val_precision_1: 0.9107 - val_recall_1: 0.9015\n","Epoch 7: early stopping\n","\u001b[94m\n","Training time: 00:32:28 | 1948.93 seconds\n"]}]},{"cell_type":"code","source":["log_data = pd.read_csv(os.path.join(DAVIDSON_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), sep=\",\", engine=\"python\")\n","display_train_report_and_f1_score(log_data)\n","plot_train_data(log_data, train_metric=\"accuracy\", validation_metric=\"val_accuracy\")\n","plot_train_data(log_data, train_metric=\"loss\", validation_metric=\"val_loss\")\n","plot_train_data(log_data, train_metric=\"precision\", validation_metric=\"val_precision\")\n","plot_train_data(log_data, train_metric=\"recall\", validation_metric=\"val_recall\")\n","plt.show()"],"metadata":{"id":"v7-HToUwQRO-","executionInfo":{"status":"error","timestamp":1654037743233,"user_tz":-180,"elapsed":15,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"colab":{"base_uri":"https://localhost:8080/","height":380},"outputId":"279d9afa-2965-49fd-8447-d94797f364ec"},"execution_count":15,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b9d171dded14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDAVIDSON_MODEL_LOGS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{MODEL_FILE_NAME}.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay_train_report_and_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_precision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-094c9cce9586>\u001b[0m in \u001b[0;36mdisplay_train_report_and_f1_score\u001b[0;34m(csv_data)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_train_report_and_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextFileReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['precision', 'recall'] not in index\""]}]},{"cell_type":"code","source":["predictions = model.predict(test_data)\n","\n","for prediction in predictions:\n","    for index, pred_class in enumerate(prediction):\n","        if pred_class == max(prediction):\n","            prediction[index] = 1\n","        else:\n","            prediction[index] = 0\n","\n","print(predictions)\n","print(test_labels)\n","print(len(predictions), len(test_labels))\n","print(type(test_labels), type(predictions))\n","\n","print(f\"\\n{classification_report(test_labels, predictions)}\")\n"],"metadata":{"id":"qfhVMNx7aoDF","executionInfo":{"status":"ok","timestamp":1654037804891,"user_tz":-180,"elapsed":15880,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddc7cca2-0e25-47d5-a660-43ec977cae91"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," ...\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n","[[0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," ...\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n","2479 2479\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.36      0.38       149\n","           1       0.93      0.93      0.93      1872\n","           2       0.84      0.87      0.86       458\n","\n","   micro avg       0.89      0.89      0.89      2479\n","   macro avg       0.73      0.72      0.72      2479\n","weighted avg       0.88      0.89      0.88      2479\n"," samples avg       0.89      0.89      0.89      2479\n","\n"]}]}]}