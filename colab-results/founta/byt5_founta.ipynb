{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"byt5_founta.ipynb","provenance":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyOgGvlQlDBteIsFRFrD1xAi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOYpYNiKKk4K","executionInfo":{"status":"ok","timestamp":1653988857599,"user_tz":-180,"elapsed":5552,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"24db0967-784e-4f7e-f520-9c52b68b7987"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from keras.models import load_model\n","from keras.metrics import Precision, Recall\n","from keras.callbacks import CSVLogger, EarlyStopping\n","from keras.utils import np_utils\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","!pip install transformers\n","!pip install sentencepiece\n","from transformers import AutoTokenizer, TFT5ForConditionalGeneration, PreTrainedTokenizerFast, PreTrainedTokenizerBase, \\\n","    TFAutoModel"]},{"cell_type":"code","source":["import nltk\n","import matplotlib.pyplot as plt\n","\n","from re import sub\n","from pandas import DataFrame\n","from tabulate import tabulate\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from typing import Union, List, Tuple\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from pandas.io.parsers import TextFileReader\n","from collections import Counter\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","__STOPWORDS = stopwords.words(\"english\")\n","STEMMER = PorterStemmer()\n","LEMMATIZER = WordNetLemmatizer()\n","\n","\n","def process_data(text: str, do_stemming: bool = False, do_lemmas: bool = False, do_lowercase: bool = False) -> str:\n","    \"\"\"\n","    @param text: The text to process. It will remove the money amounts, retweets, links,\n","                 hashtags, punctuation and it will lowercase all the words\n","    @param do_stemming: Steam words to have less in vocabulary if set to true\n","    @param do_lowercase: Lowercase input text if set to true\n","    @param do_lemmas: Lemmatize word to be at a dictionary representation if true\n","    @return: The new processed text as a list of words\n","    \"\"\"\n","    text = sub(r\" +\", \" \", text)\n","    text = sub(r\"\\S@\\S\\s?\", \"\", text)\n","    text = sub(r\"[0-9]+(?:.[0-9]+){3}\", \"\", text)\n","    text = sub(r\"\\$\\w*\", \"\", text)\n","    text = sub(r\"(RT)+\", \"\", text)\n","    text = sub(r\"(lt)+\", \"\", text)\n","    text = sub(r\"(gt)+\", \"\", text)\n","    text = sub(r\"@[a-zA-Z0-9\\_]+\", \"\", text)\n","    text = sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\", \"\", text)\n","    text = sub(r\"#\", \"\", text)\n","    text = sub(r\"\\d+\", \"\", text)\n","    text = sub(\"[^A-Za-z0-9]+\", \" \", text)  # also removes special characters since they are not alphanumeric\n","    if do_lowercase:\n","        text = text.lower()\n","\n","    # tokenized_data = __TOKENIZER.tokenize(text)\n","    tokenized_data = word_tokenize(text)\n","    processed_data = []\n","\n","    for word in tokenized_data:\n","        if word not in __STOPWORDS and word not in punctuation:\n","            if do_stemming and not do_lemmas:\n","                word = STEMMER.stem(word)\n","            elif do_lemmas and not do_stemming:\n","                word = LEMMATIZER.lemmatize(word)\n","            elif do_lemmas and do_stemming:\n","                raise ValueError(\n","                    f\"Can't do both lemmatizing and stemming. Values for do_lemmas={do_lemmas} \"\n","                    f\"and do_stemming={do_stemming} cannot be true for both.\"\n","                )\n","            processed_data.append(word)\n","\n","    return \" \".join(processed_data)\n","\n","\n","def plot_train_data(csv_data: Union[TextFileReader, DataFrame], train_metric: str, validation_metric: str) -> None:\n","    plt.figure()\n","    plt.plot(csv_data[train_metric], color=\"blueviolet\")\n","    plt.plot(csv_data[validation_metric], color=\"green\")\n","    plt.title(f\"{train_metric.capitalize()} over epochs\")\n","    plt.legend([\"train\", \"validation\"])\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(train_metric)\n","    plt.grid(visible=True)\n","\n","\n","def display_train_report_and_f1_score(csv_data: Union[TextFileReader, DataFrame]) -> None:\n","    headers = [\"epoch\", \"accuracy\", \"loss\", \"precision\", \"recall\"]\n","    train_rep = csv_data[csv_data[\"epoch\"] == len(csv_data) - 1][headers]\n","    precision = train_rep[\"precision\"].iloc[0]\n","    recall = train_rep[\"recall\"].iloc[0]\n","    f1_score = 2 * precision * recall / (precision + recall)\n","    train_rep[\"f1_score\"] = f1_score\n","    rep_data = train_rep.values.tolist()\n","    print('\\033[92m')\n","    print(\"╒═════════════════╕\")\n","    print(\"│ Training Report │\")\n","    print(tabulate(rep_data, headers=[header.capitalize() for header in headers] + [\"F1 Score\"], tablefmt=\"fancy_grid\"))\n","\n","\n","def display_readable_time(start_time: float, end_time: float) -> None:\n","    minutes = (end_time - start_time) / 60\n","    ss = (end_time - start_time) % 60\n","    hh = minutes / 60\n","    mm = minutes % 60\n","    print('\\033[94m')\n","    print(f\"Training time: %02d:%02d:%02d | {round(end_time - start_time, 2)} seconds\" % (hh, mm, ss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExbE3Co_Urpd","executionInfo":{"status":"ok","timestamp":1653988858123,"user_tz":-180,"elapsed":530,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"03f26d98-1772-4901-96c4-b399515d37bc"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_o7fsNaUtqY","executionInfo":{"status":"ok","timestamp":1653988860282,"user_tz":-180,"elapsed":2163,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"2d3e67a3-79b6-49fa-a790-7ef29e70cacf"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!mkdir train_local\n","# !wget /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","!unzip /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip -d /content/train_local"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKIoBlSHUuKH","executionInfo":{"status":"ok","timestamp":1653988971897,"user_tz":-180,"elapsed":111645,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"31a1a0c7-5d0e-4281-8e7b-bf15c79f9231"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘train_local’: File exists\n","Archive:  /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","replace /content/train_local/dynamically-hate-vidgen/Dynamically Generated Hate Dataset - annotation guidelines.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: None\n"]}]},{"cell_type":"code","source":["# TODO: Update these with the google stuff\n","FOUNTA_DIR = os.path.join(\"/content/train_local\", \"large-founta\")\n","TRAIN_SET_PATH = os.path.join(FOUNTA_DIR, \"train.tsv\")\n","VAL_SET_PATH = os.path.join(FOUNTA_DIR, \"dev.tsv\")\n","TEST_SET_PATH = os.path.join(FOUNTA_DIR, \"test.tsv\")\n","FOUNTA_MODEL_LOGS_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/logs\", \"founta\")\n","FOUNTA_MODEL_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/models\", \"founta\")\n","\n","MODEL_FILE_NAME = \"byt5_large_founta\"\n","BYT5_TYPE = \"Narrativa/byt5-base-tweet-hate-detection\"  # Narrativa/byt5-base-tweet-hate-detection\n","\n","# Clean: 48 | No lowercase: 36 | Lowercase: 30 | Lowercase & Stemming: 30 | Lowercase & Lemmas: 30\n","MAX_PADDING_LENGTH = 30\n","LEARNING_RATE = 2e-5\n","BATCH_SIZE = 32\n","EPOCHS = 10"],"metadata":{"id":"3l6WyimXUvsC","executionInfo":{"status":"ok","timestamp":1653988971898,"user_tz":-180,"elapsed":5,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def encode_tweet(tweet: str, byt5_tokenizer: Union[PreTrainedTokenizerFast, PreTrainedTokenizerBase]):\n","    return byt5_tokenizer(\n","        # tweet,\n","        # process_data(tweet),\n","        process_data(tweet, do_stemming=False, do_lemmas=False, do_lowercase=True),\n","        max_length=MAX_PADDING_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors='pt'\n","    )\n","\n","\n","def encode_tweets(tweets_text: list, tweets_labels, byt5_tokenizer: Union[PreTrainedTokenizerFast, PreTrainedTokenizerBase]):\n","    if tweets_labels is not None:\n","        assert len(tweets_text) == len(tweets_labels), f\"Features and labels must have the same lengths. \" \\\n","                                                       f\"Your input ({len(tweets_text)}, {len(tweets_labels)})\"\n","\n","    input_ids = []\n","    attention_masks = []\n","\n","    for tweet in tweets_text:\n","        tweet_for_byt5 = encode_tweet(tweet, byt5_tokenizer)\n","        input_ids.append(np.array(tweet_for_byt5[\"input_ids\"]))\n","        attention_masks.append(np.array(tweet_for_byt5[\"attention_mask\"]))\n","\n","    if tweets_labels is not None:\n","        assert len(input_ids) == len(attention_masks) == len(tweets_labels), \\\n","            \"Arrays must have the same length.\"\n","        return np.array(input_ids), np.array(attention_masks), np.array(tweets_labels)\n","\n","    return np.array(input_ids), np.array(attention_masks)\n","\n","\n","def generate_byt5_dict(input_ids, attention_mask) -> dict:\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask\n","    }\n","\n","\n","def byt5_tuning(byt5_type: str = BYT5_TYPE):\n","    input_ids = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"input_ids\", dtype=\"int32\")\n","    attention_masks = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"attention_mask\", dtype=\"int32\")\n","\n","\n","    byt5_model = TFAutoModel.from_pretrained(byt5_type, from_pt=True)\n","    print(\"REEEEEEEEEEEEEEEEEEEEE\")\n","    encodings = byt5_model(input_ids=input_ids, attention_mask=attention_masks)[0]\n","    # encodings = byt5_model.get_input_embeddings()[0]\n","    print(encodings)\n","    last_encoding = tf.squeeze(encodings[:, -1:, :], axis=1)\n","    # last_encoding = tf.keras.layers.Dropout(0.1)(last_encoding)\n","\n","    outputs = tf.keras.layers.Dense(4, activation=\"softmax\", name=\"outputs\")(last_encoding)\n","\n","    temp_model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=outputs)\n","    temp_model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"accuracy\", Precision(), Recall()]\n","    )\n","\n","    return temp_model\n","\n","\n","def convert_labels_to_numerical(labels: list):\n","    # Transform labels to numerical value\n","    for index, label in enumerate(labels):\n","        if label == \"normal\":\n","            labels[index] = 0\n","        elif label == \"spam\":\n","            labels[index] = 1\n","        elif label == \"abusive\":\n","            labels[index] = 2\n","        elif label == \"hateful\":\n","            labels[index] = 3\n","        else:\n","            raise ValueError(\"Class column must have only 'normal', 'spam', 'abusive' or 'hateful' values\")\n","\n","    return labels\n"],"metadata":{"id":"DkoKH4fKNONX","executionInfo":{"status":"ok","timestamp":1653991126018,"user_tz":-180,"elapsed":962,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(BYT5_TYPE)\n","\n","train_df = pd.read_csv(TRAIN_SET_PATH, sep=\"\\t\", header=0)\n","val_df = pd.read_csv(VAL_SET_PATH, sep=\"\\t\", header=0)\n","test_df = pd.read_csv(TEST_SET_PATH, sep=\"\\t\", header=0)\n","\n","train_texts = train_df[\"sentence\"].tolist()\n","val_texts = val_df[\"sentence\"].tolist()\n","test_texts = test_df[\"sentence\"].tolist()\n","\n","train_labels = convert_labels_to_numerical(train_df[\"class\"].tolist())\n","val_labels = convert_labels_to_numerical(val_df[\"class\"].tolist())\n","test_labels = convert_labels_to_numerical(test_df[\"class\"].tolist())\n","\n","train_labels = np_utils.to_categorical(train_labels)\n","val_labels = np_utils.to_categorical(val_labels)\n","test_labels = np_utils.to_categorical(test_labels)\n","\n","train_ids, train_masks, train_labels = encode_tweets(\n","    tweets_text=train_texts,\n","    tweets_labels=train_labels,\n","    byt5_tokenizer=tokenizer\n",")\n","validation_ids, validation_masks, val_labels = encode_tweets(\n","    tweets_text=val_texts,\n","    tweets_labels=val_labels,\n","    byt5_tokenizer=tokenizer\n",")\n","test_ids, test_masks, test_labels = encode_tweets(\n","    tweets_text=test_texts,\n","    tweets_labels=test_labels,\n","    byt5_tokenizer=tokenizer\n",")\n","\n","train_data = generate_byt5_dict(train_ids, train_masks)\n","validation_data = (generate_byt5_dict(validation_ids, validation_masks), val_labels)\n","test_data = generate_byt5_dict(test_ids, test_masks)\n"],"metadata":{"id":"zihIfNQkKndA","executionInfo":{"status":"ok","timestamp":1653989194633,"user_tz":-180,"elapsed":50051,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model = byt5_tuning()\n","print(model.summary())\n","early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=2, restore_best_weights=True)\n","csv_logger = CSVLogger(os.path.join(FOUNTA_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), separator=\",\",\n","                        append=False)\n","start_time = time.time()\n","hist = model.fit(train_data, train_labels, validation_data=validation_data, epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                  callbacks=[csv_logger, early_stop])\n","end_time = time.time()\n","model.save(os.path.join(FOUNTA_MODEL_PATH, f\"{MODEL_FILE_NAME}.h5\"))\n","display_readable_time(start_time=start_time, end_time=end_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"pOqsbiuZOu_G","executionInfo":{"status":"error","timestamp":1653991136382,"user_tz":-180,"elapsed":8188,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"baa191a2-c843-4561-d473-4594e3272dcc"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5Model: ['lm_head.weight', 'encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n","- This IS expected if you are initializing TFT5Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFT5Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFT5Model were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["REEEEEEEEEEEEEEEEEEEEE\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-3111a8072a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyt5_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m csv_logger = CSVLogger(os.path.join(FOUNTA_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), separator=\",\",\n\u001b[1;32m      5\u001b[0m                         append=False)\n","\u001b[0;32m<ipython-input-51-279fed09ec4e>\u001b[0m in \u001b[0;36mbyt5_tuning\u001b[0;34m(byt5_type)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"REEEEEEEEEEEEEEEEEEEEE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# encodings = byt5_model(input_ids=input_ids, attention_mask=attention_masks)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mlast_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'TFSharedEmbeddings' object is not subscriptable"]}]},{"cell_type":"code","source":["log_data = pd.read_csv(os.path.join(FOUNTA_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), sep=\",\", engine=\"python\")\n","display_train_report_and_f1_score(log_data)\n","plot_train_data(log_data, train_metric=\"accuracy\", validation_metric=\"val_accuracy\")\n","plot_train_data(log_data, train_metric=\"loss\", validation_metric=\"val_loss\")\n","plot_train_data(log_data, train_metric=\"precision\", validation_metric=\"val_precision\")\n","plot_train_data(log_data, train_metric=\"recall\", validation_metric=\"val_recall\")\n","plt.show()"],"metadata":{"id":"crjEgWQZVBxQ","executionInfo":{"status":"aborted","timestamp":1653989033103,"user_tz":-180,"elapsed":13,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict(test_data)\n","\n","for prediction in predictions:\n","    for index, pred_class in enumerate(prediction):\n","        if pred_class == max(prediction):\n","            prediction[index] = 1\n","        else:\n","            prediction[index] = 0\n","\n","print(predictions)\n","print(test_labels)\n","print(len(predictions), len(test_labels))\n","print(type(test_labels), type(predictions))\n","\n","print(f\"\\n{classification_report(test_labels, predictions)}\")"],"metadata":{"id":"wJZwZ5W7VDko","executionInfo":{"status":"aborted","timestamp":1653989033512,"user_tz":-180,"elapsed":422,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":null,"outputs":[]}]}