{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta_founta.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyNPYySsVgS2wM2vluGzheaP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fa-AmHtICVOb","executionInfo":{"status":"ok","timestamp":1653922371555,"user_tz":-180,"elapsed":20538,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"565a0161-f378-4cee-d323-9feef58fa860"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from keras.models import load_model\n","from keras.metrics import Precision, Recall\n","from keras.callbacks import CSVLogger, EarlyStopping\n","from keras.utils import np_utils\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","\n","!pip install transformers\n","!pip install sentencepiece\n","from transformers import PreTrainedTokenizerBase, RobertaTokenizer, TFRobertaModel"]},{"cell_type":"code","source":["import nltk\n","import matplotlib.pyplot as plt\n","\n","from re import sub\n","from pandas import DataFrame\n","from tabulate import tabulate\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from typing import Union, List, Tuple\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from pandas.io.parsers import TextFileReader\n","from collections import Counter\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","__STOPWORDS = stopwords.words(\"english\")\n","STEMMER = PorterStemmer()\n","LEMMATIZER = WordNetLemmatizer()\n","\n","\n","def process_data(text: str, do_stemming: bool = False, do_lemmas: bool = False, do_lowercase: bool = False) -> str:\n","    \"\"\"\n","    @param text: The text to process. It will remove the money amounts, retweets, links,\n","                 hashtags, punctuation and it will lowercase all the words\n","    @param do_stemming: Steam words to have less in vocabulary if set to true\n","    @param do_lowercase: Lowercase input text if set to true\n","    @param do_lemmas: Lemmatize word to be at a dictionary representation if true\n","    @return: The new processed text as a list of words\n","    \"\"\"\n","    text = sub(r\" +\", \" \", text)\n","    text = sub(r\"\\S@\\S\\s?\", \"\", text)\n","    text = sub(r\"[0-9]+(?:.[0-9]+){3}\", \"\", text)\n","    text = sub(r\"\\$\\w*\", \"\", text)\n","    text = sub(r\"(RT)+\", \"\", text)\n","    text = sub(r\"(lt)+\", \"\", text)\n","    text = sub(r\"(gt)+\", \"\", text)\n","    text = sub(r\"@[a-zA-Z0-9\\_]+\", \"\", text)\n","    text = sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\", \"\", text)\n","    text = sub(r\"#\", \"\", text)\n","    text = sub(r\"\\d+\", \"\", text)\n","    text = sub(\"[^A-Za-z0-9]+\", \" \", text)  # also removes special characters since they are not alphanumeric\n","    if do_lowercase:\n","        text = text.lower()\n","\n","    # tokenized_data = __TOKENIZER.tokenize(text)\n","    tokenized_data = word_tokenize(text)\n","    processed_data = []\n","\n","    for word in tokenized_data:\n","        if word not in __STOPWORDS and word not in punctuation:\n","            if do_stemming and not do_lemmas:\n","                word = STEMMER.stem(word)\n","            elif do_lemmas and not do_stemming:\n","                word = LEMMATIZER.lemmatize(word)\n","            elif do_lemmas and do_stemming:\n","                raise ValueError(\n","                    f\"Can't do both lemmatizing and stemming. Values for do_lemmas={do_lemmas} \"\n","                    f\"and do_stemming={do_stemming} cannot be true for both.\"\n","                )\n","            processed_data.append(word)\n","\n","    return \" \".join(processed_data)\n","\n","\n","def plot_train_data(csv_data: Union[TextFileReader, DataFrame], train_metric: str, validation_metric: str) -> None:\n","    plt.figure()\n","    plt.plot(csv_data[train_metric], color=\"blueviolet\")\n","    plt.plot(csv_data[validation_metric], color=\"green\")\n","    plt.title(f\"{train_metric.capitalize()} over epochs\")\n","    plt.legend([\"train\", \"validation\"])\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(train_metric)\n","    plt.grid(visible=True)\n","\n","\n","def display_train_report_and_f1_score(csv_data: Union[TextFileReader, DataFrame]) -> None:\n","    headers = [\"epoch\", \"accuracy\", \"loss\", \"precision\", \"recall\"]\n","    train_rep = csv_data[csv_data[\"epoch\"] == len(csv_data) - 1][headers]\n","    precision = train_rep[\"precision\"].iloc[0]\n","    recall = train_rep[\"recall\"].iloc[0]\n","    f1_score = 2 * precision * recall / (precision + recall)\n","    train_rep[\"f1_score\"] = f1_score\n","    rep_data = train_rep.values.tolist()\n","    print('\\033[92m')\n","    print(\"╒═════════════════╕\")\n","    print(\"│ Training Report │\")\n","    print(tabulate(rep_data, headers=[header.capitalize() for header in headers] + [\"F1 Score\"], tablefmt=\"fancy_grid\"))\n","\n","\n","def display_readable_time(start_time: float, end_time: float) -> None:\n","    minutes = (end_time - start_time) / 60\n","    ss = (end_time - start_time) % 60\n","    hh = minutes / 60\n","    mm = minutes % 60\n","    print('\\033[94m')\n","    print(f\"Training time: %02d:%02d:%02d | {round(end_time - start_time, 2)} seconds\" % (hh, mm, ss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9H6BKMiY9FE","executionInfo":{"status":"ok","timestamp":1653922374090,"user_tz":-180,"elapsed":2540,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"190894de-fbfe-4102-cd34-7aec05fd38e0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/omw-1.4.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4y4JAg_ZR8d","executionInfo":{"status":"ok","timestamp":1653922402412,"user_tz":-180,"elapsed":28326,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"a4345a6d-5549-4ae9-bf89-555ad459c7da"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!mkdir train_local\n","# !wget /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","!unzip /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip -d /content/train_local"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJumPHUiZWr8","executionInfo":{"status":"ok","timestamp":1653922404709,"user_tz":-180,"elapsed":2303,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"92551c19-4c0a-48d8-a1a9-37588ddcb7a0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/dataset/abusive-language/vidgen-wassem-davidson-founta-zampieri.zip\n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically Generated Hate Dataset - annotation guidelines.pdf  \n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically_Generated_Hate_Dataset_v0.2.2.csv  \n","  inflating: /content/train_local/dynamically-hate-vidgen/Dynamically_Generated_Hate_Dataset_v0.2.3.csv  \n","  inflating: /content/train_local/dynamically-hate-vidgen/README.md  \n","  inflating: /content/train_local/hateful-wassem/README.md  \n","  inflating: /content/train_local/hateful-wassem/wassem_hovy_naacl.tsv  \n","   creating: /content/train_local/hatespeech-davidson/data/\n","  inflating: /content/train_local/hatespeech-davidson/data/labeled_data.csv  \n","  inflating: /content/train_local/hatespeech-davidson/data/labeled_data.p  \n","  inflating: /content/train_local/hatespeech-davidson/data/readme.md  \n","  inflating: /content/train_local/hatespeech-davidson/LICENSE  \n","  inflating: /content/train_local/hatespeech-davidson/README.md  \n","  inflating: /content/train_local/large-founta/dev.tsv  \n","  inflating: /content/train_local/large-founta/README.md  \n","  inflating: /content/train_local/large-founta/test.tsv  \n","  inflating: /content/train_local/large-founta/train.tsv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_test_data_v0.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_test_data_v1.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_train_data_v0.csv  \n","  inflating: /content/train_local/olid-zampieri/cleaned_train_data_v1.csv  \n","  inflating: /content/train_local/olid-zampieri/olid-annotation.txt  \n","  inflating: /content/train_local/olid-zampieri/olid-training-v1.0.tsv  \n","  inflating: /content/train_local/olid-zampieri/README.txt  \n","  inflating: /content/train_local/olid-zampieri/testset-levela.tsv  \n"]}]},{"cell_type":"code","source":["# TODO: Update these with the google stuff\n","FOUNTA_DIR = os.path.join(\"/content/train_local\", \"large-founta\")\n","TRAIN_SET_PATH = os.path.join(FOUNTA_DIR, \"train.tsv\")\n","VAL_SET_PATH = os.path.join(FOUNTA_DIR, \"dev.tsv\")\n","TEST_SET_PATH = os.path.join(FOUNTA_DIR, \"test.tsv\")\n","FOUNTA_MODEL_LOGS_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/logs\", \"founta\")\n","FOUNTA_MODEL_PATH = os.path.join(\"/content/drive/MyDrive/nlp-models/abusive-language/models\", \"founta\")\n","\n","MODEL_FILE_NAME = \"roberta_large_founta\"\n","ROBERTA_TYPE = \"roberta-large\"  # roberta-base | roberta-large | roberta-large-mnli\n","\n","# Clean: 48 | No lowercase: 36 | Lowercase: 30 | Lowercase & Stemming: 30 | Lowercase & Lemmas: 30\n","MAX_PADDING_LENGTH = 30\n","LEARNING_RATE = 2e-5\n","BATCH_SIZE = 32\n","EPOCHS = 10"],"metadata":{"id":"GrcgMWcHY9xb","executionInfo":{"status":"ok","timestamp":1653922404710,"user_tz":-180,"elapsed":4,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def encode_tweet(tweet: str, roberta_tokenizer: PreTrainedTokenizerBase):\n","    return roberta_tokenizer.encode_plus(\n","        # tweet,\n","        # process_data(tweet),\n","        process_data(tweet, do_stemming=False, do_lemmas=False, do_lowercase=True),\n","        add_special_tokens=True,\n","        max_length=MAX_PADDING_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_attention_mask=True\n","    )\n","\n","\n","def encode_tweets(tweets_text: list, tweets_labels, roberta_tokenizer: PreTrainedTokenizerBase):\n","    if tweets_labels is not None:\n","        assert len(tweets_text) == len(tweets_labels), f\"Features and labels must have the same lengths. \" \\\n","                                                       f\"Your input ({len(tweets_text)}, {len(tweets_labels)})\"\n","\n","    input_ids = []\n","    attention_masks = []\n","\n","    for tweet in tweets_text:\n","        tweet_for_roberta = encode_tweet(tweet, roberta_tokenizer)\n","        input_ids.append(tweet_for_roberta[\"input_ids\"])\n","        attention_masks.append(tweet_for_roberta[\"attention_mask\"])\n","\n","    if tweets_labels is not None:\n","        assert len(input_ids) == len(attention_masks) == len(tweets_labels), \\\n","            \"Arrays must have the same length.\"\n","        return np.array(input_ids), np.array(attention_masks), np.array(tweets_labels)\n","\n","    return np.array(input_ids), np.array(attention_masks)\n","\n","\n","def generate_roberta_dict(input_ids, attention_mask) -> dict:\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask\n","    }\n","\n","\n","def roberta_tuning(roberta_type: str = ROBERTA_TYPE):\n","    input_ids = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"input_ids\", dtype=\"int32\")\n","    attention_masks = tf.keras.Input(shape=(MAX_PADDING_LENGTH,), name=\"attention_mask\", dtype=\"int32\")\n","\n","    roberta_model = TFRobertaModel.from_pretrained(roberta_type)\n","    encodings = roberta_model(input_ids=input_ids, attention_mask=attention_masks)[0]\n","    last_encoding = tf.squeeze(encodings[:, -1:, :], axis=1)\n","    # last_encoding = tf.keras.layers.Dropout(0.1)(last_encoding)\n","\n","    outputs = tf.keras.layers.Dense(4, activation=\"softmax\", name=\"outputs\")(last_encoding)\n","\n","    temp_model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=outputs)\n","    temp_model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"accuracy\", Precision(), Recall()]\n","    )\n","\n","    return temp_model\n","\n","\n","def convert_labels_to_numerical(labels: list):\n","    # Transform labels to numerical value\n","    for index, label in enumerate(labels):\n","        if label == \"normal\":\n","            labels[index] = 0\n","        elif label == \"spam\":\n","            labels[index] = 1\n","        elif label == \"abusive\":\n","            labels[index] = 2\n","        elif label == \"hateful\":\n","            labels[index] = 3\n","        else:\n","            raise ValueError(\"Class column must have only 'normal', 'spam', 'abusive' or 'hateful' values\")\n","\n","    return labels"],"metadata":{"id":"lH5r5U4tZtVg","executionInfo":{"status":"ok","timestamp":1653925603547,"user_tz":-180,"elapsed":190,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_TYPE, do_lower_case=True)\n","\n","train_df = pd.read_csv(TRAIN_SET_PATH, sep=\"\\t\", header=0)\n","val_df = pd.read_csv(VAL_SET_PATH, sep=\"\\t\", header=0)\n","test_df = pd.read_csv(TEST_SET_PATH, sep=\"\\t\", header=0)\n","\n","train_texts = train_df[\"sentence\"].tolist()\n","val_texts = val_df[\"sentence\"].tolist()\n","test_texts = test_df[\"sentence\"].tolist()\n","\n","train_labels = convert_labels_to_numerical(train_df[\"class\"].tolist())\n","val_labels = convert_labels_to_numerical(val_df[\"class\"].tolist())\n","test_labels = convert_labels_to_numerical(test_df[\"class\"].tolist())\n","\n","train_labels = np_utils.to_categorical(train_labels)\n","val_labels = np_utils.to_categorical(val_labels)\n","test_labels = np_utils.to_categorical(test_labels)\n","\n","train_ids, train_masks, train_labels = encode_tweets(\n","    tweets_text=train_texts,\n","    tweets_labels=train_labels,\n","    roberta_tokenizer=tokenizer\n",")\n","validation_ids, validation_masks, val_labels = encode_tweets(\n","    tweets_text=val_texts,\n","    tweets_labels=val_labels,\n","    roberta_tokenizer=tokenizer\n",")\n","test_ids, test_masks, test_labels = encode_tweets(\n","    tweets_text=test_texts,\n","    tweets_labels=test_labels,\n","    roberta_tokenizer=tokenizer\n",")\n","\n","train_data = generate_roberta_dict(train_ids, train_masks)\n","validation_data = (generate_roberta_dict(validation_ids, validation_masks), val_labels)\n","test_data = generate_roberta_dict(test_ids, test_masks)"],"metadata":{"id":"XB_YncKMaZKv","executionInfo":{"status":"ok","timestamp":1653925655452,"user_tz":-180,"elapsed":47165,"user":{"displayName":"Brittle","userId":"11321800852626653451"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = roberta_tuning()\n","print(model.summary())\n","early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=2, restore_best_weights=True)\n","csv_logger = CSVLogger(os.path.join(FOUNTA_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), separator=\",\",\n","                      append=False)\n","start_time = time.time()\n","hist = model.fit(train_data, train_labels, validation_data=validation_data, epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                callbacks=[csv_logger, early_stop])\n","end_time = time.time()\n","model.save(os.path.join(FOUNTA_MODEL_PATH, f\"{MODEL_FILE_NAME}.h5\"))\n","display_readable_time(start_time=start_time, end_time=end_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"G9_FachvajAk","executionInfo":{"status":"error","timestamp":1653929517259,"user_tz":-180,"elapsed":3861809,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"28bfdc0e-eac7-4085-e18d-2a8d332e4d58"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 30)]         0           []                               \n","                                                                                                  \n"," attention_mask (InputLayer)    [(None, 30)]         0           []                               \n","                                                                                                  \n"," tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  355359744  ['input_ids[0][0]',              \n"," odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 30,                                                \n","                                1024),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 1024),                                                         \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 1, 1024)     0           ['tf_roberta_model_1[0][0]']     \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," tf.compat.v1.squeeze_1 (TFOpLa  (None, 1024)        0           ['tf.__operators__.getitem_1[0][0\n"," mbda)                                                           ]']                              \n","                                                                                                  \n"," outputs (Dense)                (None, 4)            4100        ['tf.compat.v1.squeeze_1[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 355,363,844\n","Trainable params: 355,363,844\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/10\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","2167/2167 [==============================] - 980s 439ms/step - loss: 0.5857 - accuracy: 0.7907 - precision_1: 0.8025 - recall_1: 0.7745 - val_loss: 0.5209 - val_accuracy: 0.8036 - val_precision_1: 0.8104 - val_recall_1: 0.7958\n","Epoch 2/10\n","2167/2167 [==============================] - 946s 437ms/step - loss: 0.5108 - accuracy: 0.8103 - precision_1: 0.8195 - recall_1: 0.7990 - val_loss: 0.5183 - val_accuracy: 0.8084 - val_precision_1: 0.8138 - val_recall_1: 0.8025\n","Epoch 3/10\n","2167/2167 [==============================] - 945s 436ms/step - loss: 0.4752 - accuracy: 0.8227 - precision_1: 0.8315 - recall_1: 0.8123 - val_loss: 0.5280 - val_accuracy: 0.7922 - val_precision_1: 0.8014 - val_recall_1: 0.7800\n","Epoch 4/10\n","2167/2167 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.8324 - precision_1: 0.8413 - recall_1: 0.8218Restoring model weights from the end of the best epoch: 2.\n","2167/2167 [==============================] - 945s 436ms/step - loss: 0.4444 - accuracy: 0.8324 - precision_1: 0.8413 - recall_1: 0.8218 - val_loss: 0.5818 - val_accuracy: 0.8044 - val_precision_1: 0.8102 - val_recall_1: 0.7994\n","Epoch 4: early stopping\n","\u001b[94m\n","Training time: 01:03:35 | 3815.95 seconds\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-dc44b0ce00de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOUNTA_MODEL_LOGS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{MODEL_FILE_NAME}.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdisplay_train_report_and_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplot_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplot_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-094c9cce9586>\u001b[0m in \u001b[0;36mdisplay_train_report_and_f1_score\u001b[0;34m(csv_data)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_train_report_and_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextFileReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['precision', 'recall'] not in index\""]}]},{"cell_type":"code","source":["log_data = pd.read_csv(os.path.join(FOUNTA_MODEL_LOGS_PATH, f\"{MODEL_FILE_NAME}.log\"), sep=\",\", engine=\"python\")\n","display_train_report_and_f1_score(log_data)\n","plot_train_data(log_data, train_metric=\"accuracy\", validation_metric=\"val_accuracy\")\n","plot_train_data(log_data, train_metric=\"loss\", validation_metric=\"val_loss\")\n","plot_train_data(log_data, train_metric=\"precision\", validation_metric=\"val_precision\")\n","plot_train_data(log_data, train_metric=\"recall\", validation_metric=\"val_recall\")\n","plt.show()"],"metadata":{"id":"v7-HToUwQRO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict(test_data)\n","\n","for prediction in predictions:\n","    for index, pred_class in enumerate(prediction):\n","        if pred_class == max(prediction):\n","            prediction[index] = 1\n","        else:\n","            prediction[index] = 0\n","\n","print(predictions)\n","print(test_labels)\n","print(len(predictions), len(test_labels))\n","print(type(test_labels), type(predictions))\n","\n","print(f\"\\n{classification_report(test_labels, predictions)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfhVMNx7aoDF","executionInfo":{"status":"ok","timestamp":1653929620164,"user_tz":-180,"elapsed":96376,"user":{"displayName":"Brittle","userId":"11321800852626653451"}},"outputId":"c36bd7ae-c39f-4b5c-9834-dcc793b3335b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," ...\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]]\n","[[0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," ...\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]]\n","19259 19259\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.90      0.87     10373\n","           1       0.64      0.43      0.52      2579\n","           2       0.87      0.90      0.88      5335\n","           3       0.51      0.39      0.44       972\n","\n","   micro avg       0.81      0.81      0.81     19259\n","   macro avg       0.71      0.66      0.68     19259\n","weighted avg       0.80      0.81      0.80     19259\n"," samples avg       0.81      0.81      0.81     19259\n","\n"]}]}]}